{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "import math\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "FOGne7bvx6h0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MINIMUM_SAMPLE_SIZE = 4\n",
        "MAX_TREE_DEPTH = 3\n",
        "\n",
        "class tree_node:\n",
        "    def __init__(self, training_set, attribute_list, attribute_values, tree_depth):\n",
        "        self.is_leaf = False\n",
        "        self.dataset = training_set\n",
        "        self.split_attribute = None\n",
        "        self.split = None\n",
        "        self.attribute_list = attribute_list\n",
        "        self.attribute_values = attribute_values\n",
        "        self.left_child = None\n",
        "        self.right_child = None\n",
        "        self.prediction = None\n",
        "        self.depth = tree_depth\n",
        "\n",
        "    def build(self):\n",
        "        training_set = self.dataset\n",
        "\n",
        "        if self.depth < MAX_TREE_DEPTH and len(training_set) >= MINIMUM_SAMPLE_SIZE and len(set([elem[\"species\"] for elem in training_set])) > 1:\n",
        "\n",
        "            max_gain, attribute, split = max_information_gain(self.attribute_list, self.attribute_values, training_set)\n",
        "\n",
        "\n",
        "            if max_gain > 0:\n",
        "\n",
        "                self.split = split\n",
        "                self.split_attribute = attribute\n",
        "\n",
        "\n",
        "                training_set_left = [elem for elem in training_set if elem[attribute] < split]\n",
        "                training_set_right = [elem for elem in training_set if elem[attribute] >= split]\n",
        "                self.left_child = tree_node(training_set_left, self.attribute_list, self.attribute_values, self.depth + 1)\n",
        "                self.right_child = tree_node(training_set_right, self.attribute_list, self.attribute_values, self.depth + 1)\n",
        "                self.left_child.build()\n",
        "                self.right_child.build()\n",
        "            else:\n",
        "                self.is_leaf = True\n",
        "        else:\n",
        "            self.is_leaf = True\n",
        "\n",
        "        if self.is_leaf:\n",
        "\n",
        "            setosa_count = versicolor_count = virginica_count = 0\n",
        "            for elem in training_set:\n",
        "                if elem[\"species\"] == \"Iris-setosa\":\n",
        "                    setosa_count += 1\n",
        "                elif elem[\"species\"] == \"Iris-versicolor\":\n",
        "                    versicolor_count += 1\n",
        "                else:\n",
        "                    virginica_count += 1\n",
        "\n",
        "    def predict(self, sample):\n",
        "        if self.is_leaf:\n",
        "            return self.prediction\n",
        "        else:\n",
        "            if sample[self.split_attribute] < self.split:\n",
        "                return self.left_child.predict(sample)\n",
        "            else:\n",
        "                return self.right_child.predict(sample)\n",
        "\n",
        "\n",
        "    def print(self, prefix):\n",
        "        if self.is_leaf:\n",
        "            print(\"\\t\" * self.depth + prefix + self.prediction)\n",
        "        else:\n",
        "            print(\"\\t\" * self.depth + prefix + self.split_attribute + \"<\" + str(self.split) + \"?\")\n",
        "            self.left_child.print(\"[True] \")\n",
        "            self.right_child.print(\"[False] \")"
      ],
      "metadata": {
        "id": "pjGJfZCgyLsl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ID3_tree:\n",
        "    def __init__(self):\n",
        "        self.root = None\n",
        "\n",
        "    def build(self, training_set, attribute_list, attribute_values):\n",
        "        self.root = tree_node(training_set, attribute_list, attribute_values, 0)\n",
        "        self.root.build()\n",
        "\n",
        "    def predict(self, sample):\n",
        "        return self.root.predict(sample)\n",
        "\n",
        "    def print(self):\n",
        "        print(\"----------------\")\n",
        "        print(\"DECISION TREE\")\n",
        "        self.root.print(\"\")\n",
        "        print(\"----------------\")"
      ],
      "metadata": {
        "id": "PgBM2Bu6yO8V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(dataset):\n",
        "\n",
        "    if len(dataset) == 0:\n",
        "        return 0\n",
        "\n",
        "    target_attribute_name = \"species\"\n",
        "    target_attribute_values = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
        "\n",
        "    data_entropy = 0\n",
        "    for val in target_attribute_values:\n",
        "\n",
        "        p = len([elem for elem in dataset if elem[target_attribute_name] == val]) / len(dataset)\n",
        "\n",
        "        if p > 0:\n",
        "            data_entropy += -p * math.log(p, 2)\n",
        "\n",
        "    return data_entropy"
      ],
      "metadata": {
        "id": "C8IKoEckyRMc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def info_gain(attribute_name, split, dataset):\n",
        "\n",
        "    set_smaller = [elem for elem in dataset if elem[attribute_name] < split]\n",
        "    p_smaller = len(set_smaller) / len(dataset)\n",
        "    set_greater_equals = [elem for elem in dataset if elem[attribute_name] >= split]\n",
        "    p_greater_equals = len(set_greater_equals) / len(dataset)\n",
        "\n",
        "    info_gain = entropy(dataset)\n",
        "    info_gain -= p_smaller * entropy(set_smaller)\n",
        "    info_gain -= p_greater_equals * entropy(set_greater_equals)\n",
        "\n",
        "    return info_gain"
      ],
      "metadata": {
        "id": "xAQ8Ys9iyUjS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_information_gain(attribute_list, attribute_values, dataset):\n",
        "\n",
        "    max_info_gain = 0\n",
        "    for attribute in attribute_list:\n",
        "        for split in attribute_values[attribute]:\n",
        "            split_info_gain = info_gain(attribute, split, dataset)\n",
        "            if split_info_gain >= max_info_gain:\n",
        "                max_info_gain = split_info_gain\n",
        "                max_info_gain_attribute = attribute\n",
        "                max_info_gain_split = split\n",
        "    return max_info_gain, max_info_gain_attribute, max_info_gain_split"
      ],
      "metadata": {
        "id": "ICFXLDsdyXJC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiXmPfPlHi_7",
        "outputId": "6c0acd7b-e8aa-4080-bc58-3d770e22b125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset size: 150\n",
            "training set size: 113\n",
            "test set size: 37\n",
            "accuracy on test set: 0.00%\n"
          ]
        }
      ],
      "source": [
        "def read_iris_dataset():\n",
        "    dataset = []\n",
        "    with open('IRIS.csv', newline='') as csv_file:\n",
        "        reader = csv.reader(csv_file, delimiter=',')\n",
        "        is_first = True\n",
        "        for row in reader:\n",
        "\n",
        "            instance = {}\n",
        "            if not is_first:\n",
        "                instance[\"sepal_length\"] = float(row[0])\n",
        "                instance[\"sepal_width\"] = float(row[1])\n",
        "                instance[\"petal_length\"] = float(row[2])\n",
        "                instance[\"petal_width\"] = float(row[3])\n",
        "                instance[\"species\"] = row[4]\n",
        "                dataset.append(instance)\n",
        "            is_first = False\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    dataset = read_iris_dataset()\n",
        "\n",
        "    if not dataset:\n",
        "        print('dataset is empty!')\n",
        "        exit(1)\n",
        "\n",
        "    test_set = random.sample(dataset, int(0.25 * len(dataset)))\n",
        "    test_set_dupl = test_set.copy()\n",
        "    training_set = [i for i in dataset if not i in test_set_dupl or test_set_dupl.remove(i)]\n",
        "    print('dataset size:', len(dataset))\n",
        "    print('training set size:', len(training_set))\n",
        "    print('test set size:', len(test_set))\n",
        "\n",
        "    attr_list = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
        "\n",
        "    attr_domains = {}\n",
        "    for attr in list(dataset[0].keys()):\n",
        "        attr_domain = set()\n",
        "        for s in dataset:\n",
        "            attr_domain.add(s[attr])\n",
        "        attr_domains[attr] = list(attr_domain)\n",
        "\n",
        "    dt = ID3_tree()\n",
        "    dt.build(dataset, attr_list, attr_domains)\n",
        "\n",
        "    accuracy = 0\n",
        "    for sample in test_set:\n",
        "        if sample[\"species\"] == dt.predict(sample):\n",
        "            accuracy += (1/len(test_set))\n",
        "\n",
        "    print(\"accuracy on test set: \" + \"{:.2f}\".format(accuracy * 100) + \"%\")"
      ]
    }
  ]
}